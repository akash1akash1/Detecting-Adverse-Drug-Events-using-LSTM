# -*- coding: utf-8 -*-
"""M22MA007_ASSIGNMENT_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M9PNcqTBqUq34aV5ClfzcXMXVP02vkPu
"""

import pandas as pd
import csv
import re
import numpy as np
from gensim.models import Word2Vec
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, Bidirectional
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize

import nltk
nltk.download('punkt')

pip install tensorflow

# Specify the path to your CSV file
file_path = '/content/drive/MyDrive/DRUG-AE.txt'

df=pd.read_csv(file_path,sep="|", header=None, names=["PubMed_ID", "Sentence", "Adverse_Effect", "effect_Start", "effect_End", "Drug", "drug_Start", "drug_End"])

"""labels are :
adverse-effect--->B-EF
drug---->B-d
other-->O (not an entity class)
"""

df.head()

df.info()

df.head()

df.loc[4,"Sentence"]

len(df["Adverse_Effect"].unique())

unique_adverse_effects =df["Adverse_Effect"].unique()
print(f"unique_adverse_effects are {unique_adverse_effects}")

unique_drugs = df["Drug"].unique()
print(f"unique drugs are {unique_drugs}")

"""#########lets folow IO-encoding #########"""

def replace(sentence, drug, adverse_effect):
    sentence = sentence.replace(drug, f"({drug}) ")
    sentence = sentence.replace(adverse_effect, f"({adverse_effect}) ")
    return sentence

df["sentence_tags"] = df.apply(lambda x: replace(x["Sentence"], x["Drug"], x["Adverse_Effect"]), axis=1)
# print(df)

df.head()

df.loc[0,"sentence_tags"]

def words_split(test_str):
  res = re.split(r'(?<!\()[\s,]+(?![^()]*\))', test_str)
  return res

df["sentence_words"] = df["sentence_tags"].apply(words_split)

# df.head()

print(df.loc[0,"sentence_words"])

# sentence2

def remove_parentheses(sentence_words):
    return [word.strip("()") for word in sentence_words]

df["words"] = df["sentence_words"].apply(remove_parentheses)

# df.head()

print(df.loc[3,"words"])

df.drop(columns=["effect_Start","effect_End","drug_Start","drug_End"],inplace=True)

def tag(words,drug,Adverse_Effect):
  tags=[]
  for word in words:
    if word==drug:
      tags.append("B-drug")
    elif word==Adverse_Effect:
      tags.append("B-AE")
    else:
      tags.append("O")
  return tags


df["tags"]=df.apply(lambda x: tag(x["words"], x["Drug"], x["Adverse_Effect"]), axis=1)

# df.head()

# Calculate the number of words and tags for each row
df["num_words"] = df["words"].apply(len)
df["num_tags"] = df["tags"].apply(len)

# Check if the number of words matches the number of tags
df["match"] = df["num_words"] == df["num_tags"]

# print(df)

max_value = df["num_words"].max()
print(max_value)

df_updated = df[["words", "tags"]].copy()

df_updated.head()

"""#word embedding are 2 types
1)supervised
2)self supervised ---> word2vec and glove
"""

# # Tokenize the sentences in the 'words' column if they are not already tokenized
# df_updated['words'] = df_updated['words'].apply(word_tokenize)

# # Get the list of lists of words
# word_lists = df_updated['words'].tolist()

# # Initialize an empty list to hold the trained models
# models = []

# # Train a Word2Vec model for each sublist of words
# for word_list in word_lists:
#     # Train the model on the current sublist
#     model = Word2Vec([word_list], vector_size=100, window=5, min_count=1, workers=4)
#     models.append(model)

# # Save the models if you want to use them later
# for i, model in enumerate(models):
#     model.save(f"word2vec_model_{i}.model")

# # Function to convert words to vectors using a trained model
# def words_to_vectors(words, model):
#     vectors = []
#     for word in set(words):  # Convert to set to remove duplicates
#         if word in model.wv:
#             vectors.append(model.wv[word])
#     return vectors

# # Convert all words in the dataset to vectors using the last trained model
# all_word_vectors = words_to_vectors(all_words, models[-1])
# print(f"Number of unique words with vectors: {len(all_word_vectors)}")

# from gensim.models import Word2Vec

# # Your list of words
# words = [["akash", "sai"], ["vij", "pavan"]]

# # Train a Word2Vec model on your list of words
# model = Word2Vec(words, min_count=1)

# # Get the vector for a specific word
# vector = model.wv['pavan']
# # print(vector)

words = df['words'].tolist()
tags = df['tags'].tolist()
tag_to_label = {'O': 0, 'B-drug': 1, 'B-AE': 2}
labels = [[tag_to_label[tag] for tag in tag_list] for tag_list in tags]
word2vec_model = Word2Vec(words, min_count=1)
word_embeddings = [[word2vec_model.wv[word] for word in word_list] for word_list in words]

max_sequence_length = max(len(seq) for seq in words)  # Pad sequences to the same length
padded_word_embeddings = pad_sequences(word_embeddings, maxlen=max_sequence_length, padding='post')

padded_labels = pad_sequences(labels, maxlen=max_sequence_length, padding='post')
num_classes = len(set(tag_to_label.values()))
one_hot_labels = to_categorical(padded_labels, num_classes=num_classes)

padded_word_embeddings.shape

one_hot_labels.shape

print(len(padded_word_embeddings[0]))
print(len(one_hot_labels[0]))

# Define the LSTM model
embedding_dim = 100  # Dimension of the word embeddings
hidden_units = 50    # Number of LSTM units
num_classes = 3      # Number of classes in the one-hot labels

train_losses = []
val_losses = []

model = Sequential()
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True), input_shape=(90, embedding_dim)))
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define the training loop
epochs = 10
batch_size = 32

# Split the data into training and validation sets
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(padded_word_embeddings, one_hot_labels, test_size=0.2, random_state=42)

# Train the model
for epoch in range(epochs):
    print(f'Epoch {epoch+1}/{epochs}')
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=batch_size)
    # Store training and validation loss
    train_losses.append(history.history['loss'][0])
    val_losses.append(history.history['val_loss'][0])

# Plot training and validation loss after training
plt.plot(range(1, epochs+1), train_losses, label='Training Loss')
plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_val, y_val, batch_size=batch_size)
print(f'Validation loss: {loss}, Validation accuracy: {accuracy}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# Assuming y_true and y_pred are your true labels and predicted labels respectively
# y_true and y_pred should be in the shape (num_samples, num_outputs)
# y_val should also be in the same shape as your model's output

# Convert one-hot encoded labels to class labels
y_true_classes = np.argmax(y_val, axis=-1)
y_pred_classes = np.argmax(model.predict(X_val), axis=-1)

# Flatten the arrays to treat all outputs as a single multi-class classification problem
y_true_flat = y_true_classes.flatten()
y_pred_flat = y_pred_classes.flatten()

# Calculate metrics for the flattened arrays
accuracy = accuracy_score(y_true_flat, y_pred_flat)
precision = precision_score(y_true_flat, y_pred_flat, average='weighted', zero_division=1)
recall = recall_score(y_true_flat, y_pred_flat, average='weighted', zero_division=1)
f1 = f1_score(y_true_flat, y_pred_flat, average='weighted', zero_division=1)

# Print the metrics
print("Average Metrics across all Outputs:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Hypothetical new sentence
# new_sentence = ["The", "patient", "was", "administered", "aspirin", "for", "a", "headache"]
new_sentence = ["The", "patient", "was", "prescribed", "ototoxicity", "for", "pain", "relief", "and", "experienced", "nausea", "as", "a", "possible", "side", "effect"]


# Convert the new sentence to word embeddings
new_sentence_embeddings = [word2vec_model.wv[word] for word in new_sentence if word in word2vec_model.wv]

# Pad the new sentence embeddings to the same length as your other sequences
padded_new_sentence_embeddings = pad_sequences([new_sentence_embeddings], maxlen=max_sequence_length, padding='post')

prediction = model.predict(padded_new_sentence_embeddings) # Make a prediction

# The prediction will be a probability distribution over the classes
# We take the class with the highest probability for each time step
class_labels = np.argmax(prediction, axis=-1)

# Map the class labels back to the original labels
label_to_tag = {v: k for k, v in tag_to_label.items()}
predicted_tags = [label_to_tag[label] for label in class_labels[0]]

# Print the predicted tags
print(predicted_tags)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming y_true and y_pred are your true labels and predicted probabilities respectively
# y_true should be in the shape (num_samples, num_outputs)
# y_val should also be in the same shape as your model's output

# Convert y_val to a one-hot encoded format (if it's not already)
y_val_onehot = np.eye(num_classes)[y_true_classes]

# Flatten the arrays to treat all outputs as a single multi-class classification problem
y_val_flat = y_val_onehot.reshape(-1, num_classes)
y_pred_flat = model.predict(X_val).reshape(-1, num_classes)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_val_flat[:, i], y_pred_flat[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve for each class
plt.figure()
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f'ROC curve (class {i}), AUC = {roc_auc[i]:.2f}')
plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Plot the random classifier line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""##########glove vector################"""

# download glove and unzip it in Notebook.
!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
from tensorflow.keras.initializers import Constant

# Load the GloVe embeddings
glove_file = '/content/glove.6B.100d.txt'  # Replace with the path to your GloVe file
temp_file = 'glove_word2vec.txt'  # Temporary file to load into gensim
glove2word2vec(glove_file, temp_file)
glove_model = KeyedVectors.load_word2vec_format(temp_file)

# # Create the embedding matrix
# embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, embedding_dim))
# for i, word in enumerate(word2vec_model.wv.index_to_key):
#     if word in glove_model:
#         embedding_matrix[i] = glove_model[word]

# # Define the embedding layer with the GloVe embeddings
# embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],
#                             output_dim=embedding_matrix.shape[1],
#                             embeddings_initializer=Constant(embedding_matrix),
#                             trainable=False)

from tensorflow.keras.preprocessing.text import Tokenizer

# Assuming df is your DataFrame and it has 'words' and 'tags' columns
words = df['words'].tolist()
tags = df['tags'].tolist()
tag_to_label = {'O': 0, 'B-drug': 1, 'B-AE': 2}
labels = [[tag_to_label[tag] for tag in tag_list] for tag_list in tags]

# Tokenize the words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(words)
sequences = tokenizer.texts_to_sequences(words)

# Pad sequences to the same length
max_sequence_length = max(len(seq) for seq in sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

# Pad labels to the same length
padded_labels = pad_sequences(labels, maxlen=max_sequence_length, padding='post')
num_classes = len(set(tag_to_label.values()))
one_hot_labels = to_categorical(padded_labels, num_classes=num_classes)

# Define the embedding layer with the GloVe embeddings
embedding_dim = 100  # Dimension of the GloVe embeddings
embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))
for word, i in tokenizer.word_index.items():
    if word in glove_model:
        embedding_matrix[i] = glove_model[word]

# Define the model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_sequence_length,
                    trainable=False))
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(padded_sequences, one_hot_labels, test_size=0.2, random_state=42)

# Train the model
epochs = 10
batch_size = 32
for epoch in range(epochs):
    print(f'Epoch {epoch+1}/{epochs}')
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=batch_size)
    train_losses.append(history.history['loss'][0])
    val_losses.append(history.history['val_loss'][0])

    train_losses.append(history.history['loss'][0])
    val_losses.append(history.history['val_loss'][0])


# Evaluate the model
loss, accuracy = model.evaluate(X_val, y_val, batch_size=batch_size)
print(f'Validation loss: {loss}, Validation accuracy: {accuracy}')